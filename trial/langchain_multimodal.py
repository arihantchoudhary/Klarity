# -*- coding: utf-8 -*-
"""langchain-multimodal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/alejandro-ao/47db0b8b9d00b10a96ab42dd59d90b86/langchain-multimodal.ipynb

# Multi-modal RAG with LangChain

## SetUp

Install the dependencies you need to run the notebook.
"""

# for linux
# %sudo apt-get install poppler-utils tesseract-ocr libmagic-dev

# for mac
# %brew install poppler tesseract libmagic

# Commented out IPython magic to ensure Python compatibility.
# %pip install -Uq "unstructured[all-docs]" pillow lxml pillow
# %pip install -Uq chromadb tiktoken
# %pip install -Uq langchain langchain-community langchain-openai langchain-groq
# %pip install -Uq python_dotenv

import os

# keys for the services we will use

os.environ["OPENAI_API_KEY"] = "sk-..."
os.environ["GROQ_API_KEY"] = "sk-..."
os.environ["LANGCHAIN_API_KEY"] = "sk-..."
os.environ["LANGCHAIN_TRACING_V2"] = "true"

"""## Extract the data

Extract the elements of the PDF that we will be able to use in the retrieval process. These elements can be: Text, Images, Tables, etc.

### Partition PDF tables, text, and images
"""

from unstructured.partition.pdf import partition_pdf

output_path = "./content/"
file_path = output_path + 'attention.pdf'

# Reference: https://docs.unstructured.io/open-source/core-functionality/chunking
chunks = partition_pdf(
    filename=file_path,
    infer_table_structure=True,            # extract tables
    strategy="hi_res",                     # mandatory to infer tables

    extract_image_block_types=["Image"],   # Add 'Table' to list to extract image of tables
    # image_output_dir_path=output_path,   # if None, images and tables will saved in base64

    extract_image_block_to_payload=True,   # if true, will extract base64 for API usage

    chunking_strategy="by_title",          # or 'basic'
    max_characters=10000,                  # defaults to 500
    combine_text_under_n_chars=2000,       # defaults to 0
    new_after_n_chars=6000,

    # extract_images_in_pdf=True,          # deprecated
)

# We get 2 types of elements from the partition_pdf function
set([str(type(el)) for el in chunks])

# Each CompositeElement containes a bunch of related elements.
# This makes it easy to use these elements together in a RAG pipeline.

chunks[3].metadata.orig_elements

# This is what an extracted image looks like.
# It contains the base64 representation only because we set the param extract_image_block_to_payload=True

elements = chunks[3].metadata.orig_elements
chunk_images = [el for el in elements if 'Image' in str(type(el))]
chunk_images[0].to_dict()

"""### Separate extracted elements into tables, text, and images"""

# separate tables from texts
tables = []
texts = []

for chunk in chunks:
    if "Table" in str(type(chunk)):
        tables.append(chunk)

    if "CompositeElement" in str(type((chunk))):
        texts.append(chunk)

# Get the images from the CompositeElement objects
def get_images_base64(chunks):
    images_b64 = []
    for chunk in chunks:
        if "CompositeElement" in str(type(chunk)):
            chunk_els = chunk.metadata.orig_elements
            for el in chunk_els:
                if "Image" in str(type(el)):
                    images_b64.append(el.metadata.image_base64)
    return images_b64

images = get_images_base64(chunks)

"""#### Check what the images look like"""

import base64
from IPython.display import Image, display

def display_base64_image(base64_code):
    # Decode the base64 string to binary
    image_data = base64.b64decode(base64_code)
    # Display the image
    display(Image(data=image_data))

display_base64_image(images[0])

"""## Summarize the data

Create a summary of each element extracted from the PDF. This summary will be vectorized and used in the retrieval process.

### Text and Table summaries

We don't need a multimodal model to generate the summaries of the tables and the text. I will use open source models available on Groq.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -Uq langchain-groq

from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Prompt
prompt_text = """
You are an assistant tasked with summarizing tables and text.
Give a concise summary of the table or text.

Respond only with the summary, no additionnal comment.
Do not start your message by saying "Here is a summary" or anything like that.
Just give the summary as it is.

Table or text chunk: {element}

"""
prompt = ChatPromptTemplate.from_template(prompt_text)

# Summary chain
model = ChatGroq(temperature=0.5, model="llama-3.1-8b-instant")
summarize_chain = {"element": lambda x: x} | prompt | model | StrOutputParser()

# Summarize text
text_summaries = summarize_chain.batch(texts, {"max_concurrency": 3})

# Summarize tables
tables_html = [table.metadata.text_as_html for table in tables]
table_summaries = summarize_chain.batch(tables_html, {"max_concurrency": 3})

text_summaries

"""### Image summaries

We will use gpt-4o-mini to produce the image summaries.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -Uq langchain_openai

from langchain_openai import ChatOpenAI

prompt_template = """Describe the image in detail. For context,
                  the image is part of a research paper explaining the transformers
                  architecture. Be specific about graphs, such as bar plots."""
messages = [
    (
        "user",
        [
            {"type": "text", "text": prompt_template},
            {
                "type": "image_url",
                "image_url": {"url": "data:image/jpeg;base64,{image}"},
            },
        ],
    )
]

prompt = ChatPromptTemplate.from_messages(messages)

chain = prompt | ChatOpenAI(model="gpt-4o-mini") | StrOutputParser()


image_summaries = chain.batch(images)

image_summaries

print(image_summaries[1])

"""## Load data and summaries to vectorstore

### Create the vectorstore
"""

import uuid
from langchain.vectorstores import Chroma
from langchain.storage import InMemoryStore
from langchain.schema.document import Document
from langchain.embeddings import OpenAIEmbeddings
from langchain.retrievers.multi_vector import MultiVectorRetriever

# The vectorstore to use to index the child chunks
vectorstore = Chroma(collection_name="multi_modal_rag", embedding_function=OpenAIEmbeddings())

# The storage layer for the parent documents
store = InMemoryStore()
id_key = "doc_id"

# The retriever (empty to start)
retriever = MultiVectorRetriever(
    vectorstore=vectorstore,
    docstore=store,
    id_key=id_key,
)

"""### Load the summaries and link the to the original data"""

# Add texts
doc_ids = [str(uuid.uuid4()) for _ in texts]
summary_texts = [
    Document(page_content=summary, metadata={id_key: doc_ids[i]}) for i, summary in enumerate(text_summaries)
]
retriever.vectorstore.add_documents(summary_texts)
retriever.docstore.mset(list(zip(doc_ids, texts)))

# Add tables
table_ids = [str(uuid.uuid4()) for _ in tables]
summary_tables = [
    Document(page_content=summary, metadata={id_key: table_ids[i]}) for i, summary in enumerate(table_summaries)
]
retriever.vectorstore.add_documents(summary_tables)
retriever.docstore.mset(list(zip(table_ids, tables)))

# Add image summaries
img_ids = [str(uuid.uuid4()) for _ in images]
summary_img = [
    Document(page_content=summary, metadata={id_key: img_ids[i]}) for i, summary in enumerate(image_summaries)
]
retriever.vectorstore.add_documents(summary_img)
retriever.docstore.mset(list(zip(img_ids, images)))

"""### Check retrieval"""

# Retrieve
docs = retriever.invoke(
    "who are the authors of the paper?"
)

for doc in docs:
    print(str(doc) + "\n\n" + "-" * 80)

"""## RAG pipeline"""

from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI
from base64 import b64decode


def parse_docs(docs):
    """Split base64-encoded images and texts"""
    b64 = []
    text = []
    for doc in docs:
        try:
            b64decode(doc)
            b64.append(doc)
        except Exception as e:
            text.append(doc)
    return {"images": b64, "texts": text}


def build_prompt(kwargs):

    docs_by_type = kwargs["context"]
    user_question = kwargs["question"]

    context_text = ""
    if len(docs_by_type["texts"]) > 0:
        for text_element in docs_by_type["texts"]:
            context_text += text_element.text

    # construct prompt with context (including images)
    prompt_template = f"""
    Answer the question based only on the following context, which can include text, tables, and the below image.
    Context: {context_text}
    Question: {user_question}
    """

    prompt_content = [{"type": "text", "text": prompt_template}]

    if len(docs_by_type["images"]) > 0:
        for image in docs_by_type["images"]:
            prompt_content.append(
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image}"},
                }
            )

    return ChatPromptTemplate.from_messages(
        [
            HumanMessage(content=prompt_content),
        ]
    )


chain = (
    {
        "context": retriever | RunnableLambda(parse_docs),
        "question": RunnablePassthrough(),
    }
    | RunnableLambda(build_prompt)
    | ChatOpenAI(model="gpt-4o-mini")
    | StrOutputParser()
)

chain_with_sources = {
    "context": retriever | RunnableLambda(parse_docs),
    "question": RunnablePassthrough(),
} | RunnablePassthrough().assign(
    response=(
        RunnableLambda(build_prompt)
        | ChatOpenAI(model="gpt-4o-mini")
        | StrOutputParser()
    )
)

response = chain.invoke(
    "What is the attention mechanism?"
)

print(response)

response = chain_with_sources.invoke(
    "What is multihead?"
)

print("Response:", response['response'])

print("\n\nContext:")
for text in response['context']['texts']:
    print(text.text)
    print("Page number: ", text.metadata.page_number)
    print("\n" + "-"*50 + "\n")
for image in response['context']['images']:
    display_base64_image(image)

"""## References

- [LangChain Inspiration](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb?ref=blog.langchain.dev)
- [Multivector Storage](https://python.langchain.com/docs/how_to/multi_vector/)
"""


import os
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
import base64
from datetime import datetime

# Vector embedding and indexing
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS, Chroma
from langchain.schema.document import Document

# Document processing
from unstructured.partition.pdf import partition_pdf
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

# LLM integration
from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI

# For indexing pipeline management
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import uuid

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("dense_vector_indexing")

class DocumentProcessor:
    """Handles document processing and metadata extraction"""
    
    def __init__(self, output_path: str = "./content/"):
        self.output_path = output_path
        os.makedirs(output_path, exist_ok=True)
        
    def process_pdf(self, file_path: str, extract_images: bool = True) -> Tuple[List[Any], List[Any], List[str]]:
        """Process PDF document and extract text, tables, and images"""
        logger.info(f"Processing PDF: {file_path}")
        
        chunks = partition_pdf(
            filename=file_path,
            infer_table_structure=True,
            strategy="hi_res",
            extract_image_block_types=["Image"],
            extract_image_block_to_payload=True,
            chunking_strategy="by_title",
            max_characters=10000,
            combine_text_under_n_chars=2000,
            new_after_n_chars=6000,
        )
        
        # Separate tables from texts
        tables = []
        texts = []
        
        for chunk in chunks:
            if "Table" in str(type(chunk)):
                tables.append(chunk)
            if "CompositeElement" in str(type(chunk)):
                texts.append(chunk)
                
        # Extract images from composite elements
        images = []
        if extract_images:
            for chunk in chunks:
                if "CompositeElement" in str(type(chunk)):
                    chunk_els = chunk.metadata.orig_elements
                    for el in chunk_els:
                        if "Image" in str(type(el)):
                            images.append(el.metadata.image_base64)
        
        logger.info(f"Extracted {len(texts)} text chunks, {len(tables)} tables, and {len(images)} images")
        return texts, tables, images

class ContentSummarizer:
    """Generates summaries for text, tables, and images"""
    
    def __init__(self, 
                text_model: str = "llama-3.1-8b-instant",
                image_model: str = "gpt-4o-mini",
                temperature: float = 0.5):
        self.text_model = ChatGroq(temperature=temperature, model=text_model)
        self.image_model = ChatOpenAI(model=image_model)
        
        # Text/table summarization prompt
        text_prompt_template = """
        You are an assistant tasked with summarizing tables and text.
        Give a concise summary of the table or text.

        Respond only with the summary, no additional comment.
        Do not start your message by saying "Here is a summary" or anything like that.
        Just give the summary as it is.

        Table or text chunk: {element}
        """
        self.text_prompt = ChatPromptTemplate.from_template(text_prompt_template)
        self.text_chain = {"element": lambda x: x} | self.text_prompt | self.text_model | StrOutputParser()
        
        # Image summarization prompt
        image_prompt_template = """Describe the image in detail. For context,
                      the image is part of a document. Be specific about visual elements,
                      such as graphs, diagrams, or tables if present."""
        self.image_messages = [
            (
                "user",
                [
                    {"type": "text", "text": image_prompt_template},
                    {
                        "type": "image_url",
                        "image_url": {"url": "data:image/jpeg;base64,{image}"},
                    },
                ],
            )
        ]
        self.image_prompt = ChatPromptTemplate.from_messages(self.image_messages)
        self.image_chain = self.image_prompt | self.image_model | StrOutputParser()
    
    def summarize_texts(self, texts: List[Any], max_concurrency: int = 3) -> List[str]:
        """Summarize text chunks"""
        logger.info(f"Summarizing {len(texts)} text chunks")
        return self.text_chain.batch(texts, {"max_concurrency": max_concurrency})
    
    def summarize_tables(self, tables: List[Any], max_concurrency: int = 3) -> List[str]:
        """Summarize tables"""
        logger.info(f"Summarizing {len(tables)} tables")
        tables_html = [table.metadata.text_as_html for table in tables]
        return self.text_chain.batch(tables_html, {"max_concurrency": max_concurrency})
    
    def summarize_images(self, images: List[str], max_concurrency: int = 2) -> List[str]:
        """Summarize images"""
        logger.info(f"Summarizing {len(images)} images")
        return self.image_chain.batch(images)

class DenseVectorIndexer:
    """Handles creation and management of dense vector indices"""
    
    def __init__(self, 
                collection_name: str = "dense_vector_index",
                embedding_model: str = "text-embedding-3-small",
                use_faiss: bool = False):
        self.collection_name = collection_name
        self.embedding_function = OpenAIEmbeddings(model=embedding_model)
        self.use_faiss = use_faiss
        
        # Choose vector store based on configuration
        if use_faiss:
            self.vector_store = FAISS(embedding_function=self.embedding_function)
        else:
            self.vector_store = Chroma(collection_name=collection_name, 
                                      embedding_function=self.embedding_function)
        
        # For storing document metadata and content
        self.document_store = {}
        
    def create_document(self, content: str, metadata: Dict[str, Any]) -> Document:
        """Create a Document object for indexing"""
        return Document(page_content=content, metadata=metadata)
    
    def add_documents(self, documents: List[Document]) -> None:
        """Add documents to the vector store"""
        logger.info(f"Adding {len(documents)} documents to vector store")
        self.vector_store.add_documents(documents)
    
    def store_documents_with_summaries(self, 
                                      texts: List[Any], 
                                      text_summaries: List[str],
                                      tables: List[Any], 
                                      table_summaries: List[str],
                                      images: List[str], 
                                      image_summaries: List[str]) -> None:
        """Store documents with their summaries in the vector store"""
        
        documents = []
        
        # Process text chunks
        for i, (text, summary) in enumerate(zip(texts, text_summaries)):
            doc_id = str(uuid.uuid4())
            metadata = {
                "doc_id": doc_id,
                "type": "text",
                "page_number": text.metadata.page_number if hasattr(text.metadata, "page_number") else None,
                "source_file": text.metadata.source_file if hasattr(text.metadata, "source_file") else None,
                "timestamp": datetime.now().isoformat()
            }
            
            # Store the original text in document store
            self.document_store[doc_id] = text
            
            # Create document with summary for vector indexing
            documents.append(self.create_document(summary, metadata))
        
        # Process tables
        for i, (table, summary) in enumerate(zip(tables, table_summaries)):
            doc_id = str(uuid.uuid4())
            metadata = {
                "doc_id": doc_id,
                "type": "table",
                "page_number": table.metadata.page_number if hasattr(table.metadata, "page_number") else None,
                "source_file": table.metadata.source_file if hasattr(table.metadata, "source_file") else None,
                "timestamp": datetime.now().isoformat()
            }
            
            # Store the original table in document store
            self.document_store[doc_id] = table
            
            # Create document with summary for vector indexing
            documents.append(self.create_document(summary, metadata))
        
        # Process images
        for i, (image, summary) in enumerate(zip(images, image_summaries)):
            doc_id = str(uuid.uuid4())
            metadata = {
                "doc_id": doc_id,
                "type": "image",
                "timestamp": datetime.now().isoformat()
            }
            
            # Store the original image in document store
            self.document_store[doc_id] = image
            
            # Create document with summary for vector indexing
            documents.append(self.create_document(summary, metadata))
        
        # Add all documents to vector store
        self.add_documents(documents)
        logger.info(f"Total documents stored: {len(documents)}")
    
    def similarity_search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        """Perform similarity search against the vector index"""
        logger.info(f"Performing similarity search for query: {query}")
        results = self.vector_store.similarity_search_with_score(query, k=k)
        
        # Resolve document references
        resolved_results = []
        for doc, score in results:
            doc_id = doc.metadata.get("doc_id")
            if doc_id in self.document_store:
                original_content = self.document_store[doc_id]
                resolved_results.append((original_content, score))
            else:
                resolved_results.append((doc, score))
        
        return resolved_results
    
    def hybrid_search(self, query: str, k: int = 5, alpha: float = 0.5) -> List[Tuple[Document, float]]:
        """
        Perform hybrid search combining dense vector search with BM25-like keyword matching
        Note: This is a simplified hybrid approach; full implementation would depend on the 
        specific vector store capabilities
        """
        # For Chroma, use hybrid search if available
        if not self.use_faiss and hasattr(self.vector_store, "hybrid_search"):
            return self.vector_store.hybrid_search(query, k=k, alpha=alpha)
        
        # For FAISS or if hybrid search not available, fall back to similarity search
        return self.similarity_search(query, k=k)
    
    def save_index(self, path: str) -> None:
        """Save the vector index to disk"""
        if self.use_faiss:
            self.vector_store.save_local(path)
        else:
            # Chroma persistence depends on how it was initialized
            logger.info("Chroma persistence is handled through its constructor")
    
    def load_index(self, path: str) -> None:
        """Load a vector index from disk"""
        if self.use_faiss:
            self.vector_store = FAISS.load_local(path, self.embedding_function)
        else:
            logger.info("Chroma persistence is handled through its constructor")

class RAGPipeline:
    """Retrieval-Augmented Generation pipeline integrating all components"""
    
    def __init__(self, 
                processor: DocumentProcessor,
                summarizer: ContentSummarizer,
                indexer: DenseVectorIndexer,
                llm_model: str = "gpt-4o-mini"):
        self.processor = processor
        self.summarizer = summarizer
        self.indexer = indexer
        self.llm = ChatOpenAI(model=llm_model)
    
    def process_document(self, file_path: str) -> None:
        """Process a document and add it to the index"""
        # Extract content from document
        texts, tables, images = self.processor.process_pdf(file_path)
        
        # Generate summaries
        text_summaries = self.summarizer.summarize_texts(texts)
        table_summaries = self.summarizer.summarize_tables(tables)
        image_summaries = self.summarizer.summarize_images(images)
        
        # Store documents with their summaries in the vector index
        self.indexer.store_documents_with_summaries(
            texts, text_summaries,
            tables, table_summaries,
            images, image_summaries
        )
    
    def parse_results(self, results):
        """Parse search results into images and texts"""
        images = []
        texts = []
        
        for item, score in results:
            if isinstance(item, str):  # Image base64
                images.append(item)
            else:  # Text or table
                texts.append(item)
                
        return {"images": images, "texts": texts}
    
    def build_prompt(self, kwargs):
        """Build a prompt for the LLM including context and images"""
        docs_by_type = kwargs["context"]
        user_question = kwargs["question"]

        context_text = ""
        if len(docs_by_type["texts"]) > 0:
            for text_element in docs_by_type["texts"]:
                context_text += str(text_element)

        # Construct prompt with context (including images)
        prompt_template = f"""
        Answer the question based only on the following context, which can include text, tables, and images.
        Context: {context_text}
        Question: {user_question}
        """

        prompt_content = [{"type": "text", "text": prompt_template}]

        if len(docs_by_type["images"]) > 0:
            for image in docs_by_type["images"]:
                prompt_content.append(
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image}"},
                    }
                )

        return ChatPromptTemplate.from_messages([
            ("user", prompt_content),
        ])
    
    def query(self, question: str, k: int = 5) -> str:
        """Query the RAG pipeline with a question"""
        # Retrieve relevant documents
        results = self.indexer.similarity_search(question, k=k)
        
        # Parse results
        context = self.parse_results(results)
        
        # Build chain
        chain = (
            {
                "context": lambda x: context,
                "question": RunnablePassthrough(),
            }
            | RunnableLambda(self.build_prompt)
            | self.llm
            | StrOutputParser()
        )
        
        # Generate response
        response = chain.invoke(question)
        return response
    
    def query_with_sources(self, question: str, k: int = 5) -> Dict[str, Any]:
        """Query with sources included in response"""
        # Retrieve relevant documents
        results = self.indexer.similarity_search(question, k=k)
        
        # Parse results
        context = self.parse_results(